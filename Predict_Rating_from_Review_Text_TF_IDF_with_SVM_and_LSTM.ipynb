{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Predict Rating from Review Text - TF-IDF with SVM and LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spla23/Predicting-Job-Scores/blob/main/Predict_Rating_from_Review_Text_TF_IDF_with_SVM_and_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "id": "evr4XPIgtgUj"
      },
      "source": [
        "### Predicting Glassdoor Ratings from Reviews\n",
        "\n",
        "**Frameworks**\n",
        "\n",
        "Train a machine learning model to predict the star rating assigned to a job by an employee, based on the content of the review text.\n",
        "\n",
        "We will use NLP frameworks to extract and rank the impact weights of the review text as feature input.\n",
        "We will use hot-encoded categories as our training set to determine the weights of features and our desired prediction outcome.\n",
        "\n",
        "**Problem, Hypothesis, Use Cases**\n",
        "\n",
        "Our goal is to determine which words and phrases are highly correlated with a '1 star job' rating and a '5 star' job rating in the field of data science.\n",
        "\n",
        "We will use the Glassdoor star rating system as a proxy for 'job quality', and assume a job ranked '5 stars' by an employee is a 'good job' that someone would enjoy and a '1 star job' is a bad job.\n",
        "\n",
        "We assume there are specific words and phrases that will be repeated in the review text that are the 'features' of a 'good job' (a '5 star job') and a 'bad job'.\n",
        "\n",
        "As an example, the phrase \"learned a lot during my time there\" may be associated with a '4 star' or '5 star' job rating. A '1 star' job might see a lot of employees saying \"bad manager\" in the \"cons\" section.\n",
        "\n",
        "If given some other text in context with the job, could we predict the quality of a job experience based on the words and phrases used?\n",
        "\n",
        "Could theese be used as 'red flags' or 'green light' signals to job seekers in the domain of data science?\n",
        "\n",
        "**Labels**\n",
        "\n",
        "The Star Rating Assigned to a Review: 'One Star' to 'Five Stars'\n",
        "\n",
        "**Features**\n",
        "\n",
        "words in the review text\n",
        "sentences in the review text\n",
        "\n",
        "**Dataset**\n",
        "\n",
        "Dataset comes from Kaggle https://www.kaggle.com/andresionek/data-jobs-listings-glassdoor\n",
        "Initial set is 1.48 GB with more than 160k rows and 265 columns in 15 tables\n",
        "\n",
        "Data consists of Scraped Glassdoor company listings, job listings and review text for data science related search terms, for every country:\n",
        "* data-scientist\n",
        "* software-engineer\n",
        "* data-analyst\n",
        "* research-scientist\n",
        "* business-analyst\n",
        "* product-manager\n",
        "* project-manager\n",
        "* data-engineer\n",
        "* statistician\n",
        "* dba\n",
        "* database-engineer\n",
        "* machine-learning-engineer\n",
        "\n",
        "\n",
        "*For predicting the rating of a glassdoor review, I will be using the review text and ratings data only. For prototypes, I have sliced the data thinly and will expand as time warrants. This will reduce the scope of the data significantly to reduce load and runtime.*\n",
        "\n",
        "**Model**\n",
        "\n",
        "We will be using a classification model to predicts the discrete ratings values.\n",
        "The ratings are discrete per review, so the rating prediction will be discrete.\n",
        "There are 5 possible inputs: 'One Star', 'Two Stars', 'Three Stars', 'Four Stars', 'Five Stars', \n",
        "Accordingly, there will be 5 possible prediction values.\n",
        "This model will detect words and phrases in text that are highly correlated with five rating labels and predict the rating from unlabeled text.\n",
        "\n",
        "**Method**\n",
        "\n",
        "Explore and Clean Data\n",
        "\n",
        "Extract cleaned text and ratings as input\n",
        "\n",
        "Encode word and sentence vectors/tokens\n",
        "\n",
        "Train Machine Learning Models to assign weights to inputs\n",
        "\n",
        "\n",
        "The model will be trained to learn the relationships between 'review text' and 'star rating'.\n",
        "\n",
        "\n",
        "The trained model will be apply what it's learned to unrated review text and predict the star rating for each.\n",
        "\n",
        "\n",
        "We will then evaluate the accuracy of the prediction and tune the results by experimenting with more NLP, feature extraction frameworks and machine learning models."
      ],
      "id": "evr4XPIgtgUj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEUmrLFXtgUp"
      },
      "source": [
        "## Data Cleaning, Shaping and Exploration"
      ],
      "id": "hEUmrLFXtgUp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LBGJB4stgUp"
      },
      "source": [
        "### Load data"
      ],
      "id": "3LBGJB4stgUp"
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQ9XjdwgtgUp",
        "outputId": "440dfe9c-186d-4801-bec4-5f09777919fd"
      },
      "source": [
        "#import libraries\n",
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "import re\n",
        "#import hvplot.pandas\n",
        "#import holoviews as _hv\n",
        "#from holoviews import Store\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import reuters, stopwords\n",
        "\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "\n",
        "from sklearn import tree\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, losses, preprocessing\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers.core import Activation, Dropout, Dense\n",
        "from keras.layers import Flatten, LSTM\n",
        "from keras.layers import GlobalMaxPooling1D\n",
        "from keras.models import Model\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import Input\n",
        "from keras.layers.merge import Concatenate\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "# Initial imports\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "%matplotlib inline\n",
        "\n",
        "# Needed for decision tree visualization\n",
        "import pydotplus\n",
        "from IPython.display import Image\n",
        "\n",
        "\n",
        "# Needed for decision tree visualization\n",
        "import pydotplus\n",
        "from IPython.display import Image\n",
        "\n",
        "from numpy.random import seed\n",
        "#seed(1)\n",
        "from tensorflow import random\n",
        "#random.set_seed(2)\n",
        "\n",
        "# Code to download corpora for TF-IDF\n",
        "import nltk\n",
        "nltk.download('reuters')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "#Import Libraries\n",
        "\n",
        "\n"
      ],
      "id": "qQ9XjdwgtgUp",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuZjlgpvuhg1",
        "outputId": "d8c29e5a-c374-4498-f314-f7e6e956a1cd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "PuZjlgpvuhg1",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "id": "i58-5SL6tgUr",
        "outputId": "f3794028-4a78-457d-fa13-b154ad2b4545"
      },
      "source": [
        "# Load the review data\n",
        "glassdoor_reviews_df=pd.read_csv('glassdoor_reviews.csv')\n",
        "glassdoor_reviews_df\n",
        "\n",
        "# Preview column names and info about data\n",
        "glassdoor_reviews_df.info(), glassdoor_reviews_df.shape"
      ],
      "id": "i58-5SL6tgUr",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-468ed484f126>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the review data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mglassdoor_reviews_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'glassdoor_reviews.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mglassdoor_reviews_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Preview column names and info about data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2157\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2158\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 28 fields in line 3821, saw 52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "DJywjWebtgUr"
      },
      "source": [
        "#backup and reviewer headers\n",
        "#backup1 = glassdoor_reviews_df.copy()#for testing purposes\n",
        "glassdoor_reviews_df.head()"
      ],
      "id": "DJywjWebtgUr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IG1xUg3ttgUr"
      },
      "source": [
        "#### Clean data"
      ],
      "id": "IG1xUg3ttgUr"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAR9s2_ntgUs"
      },
      "source": [
        "#delete Nans (rows without a review ID)\n",
        "glassdoor_reviews_df.dropna(subset = [\"reviews.val.id\"], inplace=True)"
      ],
      "id": "KAR9s2_ntgUs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6wBOBwutgUx"
      },
      "source": [
        "#filter for english speaking locations - eliminate non-english reviews\n",
        "glassdoor_reviews_df = glassdoor_reviews_df.set_index(\"reviews.val.reviewerLocation\")#set index as location to filter for list values\n",
        "glassdoor_reviews_df.columns"
      ],
      "id": "M6wBOBwutgUx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "OBgKL2gNtgUs"
      },
      "source": [
        "#filter for english speaking locations - eliminate non-english reviews (start with one to keep)\n",
        "locations_to_keep = ['Dublin, Co. Dublin (Ireland)',\n",
        "                     'Toronto, ON (Canada)',\n",
        "                     'Dorking, England',\n",
        "                     'Calgary, AB (Canada)',\n",
        "                     'Southampton, England',\n",
        "                     'Billingham, England',\n",
        "                     'Auckland, Auckland (New Zealand)',\n",
        "                     'Perth, Western Australia (Australia)',\n",
        "                     'Seattle, WA (US)',\n",
        "                     'Austin, TX (US)',\n",
        "                     'Montreal, QC (Canada)',\n",
        "                     'San Mateo, CA (US)',\n",
        "                     'Sandton (South Africa)',\n",
        "                     'Aurora, CO (US)',\n",
        "                     'London, England',\n",
        "                     'Madison, WI (US)',\n",
        "                     'Sydney (Australia)',\n",
        "                     'Melbourne (Australia)',\n",
        "                     'Carrick-on-Shannon (Ireland)',\n",
        "                     'Birmingham, England',\n",
        "                     'Belfast, Northern Ireland',\n",
        "                     'Vancouver, BC (Canada)',\n",
        "                     'New York, NY (US)',\n",
        "                     'Irvine, CA (US)',\n",
        "                     'Tempe, AZ (US)',\n",
        "                     'Stellenbosch (South Africa)',\n",
        "                     'Cork (Ireland)',\n",
        "                     'Ottawa, ON (Canada)',\n",
        "                     'Agawam, MA (US)',\n",
        "                     'Brackenfell (South Africa)',\n",
        "                     'Saint-Laurent, QC (Canada)',\n",
        "                     'Stockport, England',\n",
        "                     'Wexford (Ireland)',\n",
        "                     'Richmond, VA (US)',\n",
        "                     'Johannesburg (South Africa)',\n",
        "                     'Orange, CA (US)',\n",
        "                     'Bellevue, WA (US)',\n",
        "                     'Chiswick, England',\n",
        "                     'Mississauga, ON (Canada)',\n",
        "                     'Cape Town (South Africa)',\n",
        "                     'Winnipeg, MB (Canada)',\n",
        "                     'Ipswich, MA (US)',\n",
        "                     'Newbury, England',\n",
        "                     'Stevens Point, WI (US)',\n",
        "                     'Edinburgh, Scotland',\n",
        "                     'Alpharetta, GA (US)',\n",
        "                     'Arlington, VA (US)',\n",
        "                     'Edmonton, AB (Canada)',\n",
        "                     'Newcastle upon Tyne, England',\n",
        "                     'Reston, VA (US)',\n",
        "                     'Copenhagen (Denmark)',\n",
        "                     'Santa Barbara, CA (US)',\n",
        "                     'Leeds, England',\n",
        "                     'Laval, QC (Canada)',\n",
        "                     'Charlotte, NC (US)',\n",
        "                     'Miami, FL (US)',\n",
        "                     'Somerset West (South Africa)',\n",
        "                     'Basking Ridge, NJ (US)',\n",
        "                     'Winnersh, England',\n",
        "                     'Natick, MA (US)',\n",
        "                     'Los Angeles, CA (US)',\n",
        "                     'Naples, FL (US)',\n",
        "                     'Redding, CA (US)',\n",
        "                     'Albuquerque, NM (US)',\n",
        "                     'Galway (Ireland)',\n",
        "                     'Pretoria, Gauteng (South Africa)',\n",
        "                     'Wellington, Wellington (New Zealand)',\n",
        "                     'Lincoln (New Zealand)',\n",
        "                     'Palmdale, CA (US)',\n",
        "                     'Glasgow, Scotland',\n",
        "                     'Limerick, Co. Limerick (Ireland)',\n",
        "                     'Chagrin Falls, OH (US)',\n",
        "                     'Warrington, North West England, England',\n",
        "                     'San Francisco, CA (US)',\n",
        "                     'Sheffield, England',\n",
        "                     'Cameron, LA (US)',\n",
        "                     'Spring City, TN (US)',\n",
        "                     'Richmond, London, England',\n",
        "                     'Leicester, England',\n",
        "                     'Des Moines, IA (US)',\n",
        "                     'Los Alamos, NM (US)',\n",
        "                     'Burlington, MA (US)',\n",
        "                     'Sacramento, CA (US)',\n",
        "                     'Sunnyvale, CA (US)',\n",
        "                     'Durham, NC (US)',\n",
        "                     'Jacksonville, FL (US)',\n",
        "                     'Beaverton, OR (US)',\n",
        "                     'Purchase, NY (US)',\n",
        "                     'Pittsburgh, PA (US)',\n",
        "                     'San Jose, CA (US)',\n",
        "                     'Cambridge, MA (US)',\n",
        "                     'Portsmouth, NH (US)',\n",
        "                     'Stevenage, England',\n",
        "                     'Nottingham, England',\n",
        "                     'Hazelwood, England',\n",
        "                     'Redlands, CA (US)',\n",
        "                     'Guelph, ON (Canada)',\n",
        "                     'North Sydney (Australia)',\n",
        "                     'Roseville, CA (US)',\n",
        "                     'Alexandria, VA (US)',\n",
        "                     'Thames Ditton, England',\n",
        "                     'San Carlos, CA (US)',\n",
        "                     'High Wycombe, England',\n",
        "                     'Heathrow, England',\n",
        "                     'Santa Clara, CA (US)',\n",
        "                     'Cambridge, England',\n",
        "                     'Oakville, ON (Canada)',\n",
        "                     'Brisbane (Australia)',\n",
        "                     'Santa Monica, CA (US)',\n",
        "                     'Waterloo, ON (Canada)',\n",
        "                     'Lower Hutt (New Zealand)',\n",
        "                     'Huntingdon, England',\n",
        "                     'Markham, ON (Canada)',\n",
        "                     'San Antonio, TX (US)',\n",
        "                     'Lexington, MA (US)',\n",
        "                     'Wakefield, MA (US)',\n",
        "                     'Moorpark, CA (US)',\n",
        "                     'Salem, OR (US)',\n",
        "                     'Melville, NY (US)',\n",
        "                     'Pasadena, CA (US)',\n",
        "                     'Reading, PA (US)',\n",
        "                     'Brampton, ON (Canada)',\n",
        "                     'Tysons, VA (US)',\n",
        "                     'Boston, MA (US)',\n",
        "                     'Provo, UT (US)',\n",
        "                     'Lebanon, NH (US)',\n",
        "                     'Stafford, England',\n",
        "                     'North Ryde (Australia)',\n",
        "                     'Blackpool, England',\n",
        "                     'Lytham St Annes, England',\n",
        "                     'Lytham, England',\n",
        "                     'Charleston, SC (US)',\n",
        "                     'Portland, OR (US)',\n",
        "                     'Killarney (Ireland)',\n",
        "                     'Columbus, GA (US)',\n",
        "                     'Salt Lake City, UT (US)',\n",
        "                     'Orem, UT (US)',\n",
        "                     'Simi Valley, CA (US)',\n",
        "                     'Crawley, West Sussex, South East England, England',\n",
        "                     'Wilton, CT (US)',\n",
        "                     'Washington, DC (US)',\n",
        "                     'Southfield, MI (US)',\n",
        "                     'Adelaide (Australia)',\n",
        "                     'Woodcliff Lake, NJ (US)',\n",
        "                     'North York, ON (Canada)',\n",
        "                     'Welwyn Garden City, England',\n",
        "                     'Dallas, TX (US)',\n",
        "                     'Orlando, FL (US)',\n",
        "                     'Chicago, IL (US)',\n",
        "                     'Surry Hills (Australia)',\n",
        "                     'Pinelands, Western Cape (South Africa)',\n",
        "                     'Cardiff, Wales',\n",
        "                     'Malmesbury, England',\n",
        "                     'Christchurch, Canterbury (New Zealand)',\n",
        "                     'Bristol, England',\n",
        "                     'Dundee, Scotland',\n",
        "                     'Chantilly, VA (US)',\n",
        "                     'Canberra (Australia)',\n",
        "                     'Rohnert Park, CA (US)',\n",
        "                     'Bloomington, MN (US)',\n",
        "                     'Livermore, CA (US)',\n",
        "                     'Hauppauge, NY (US)',\n",
        "                     'Atlanta, GA (US)',\n",
        "                     'Kelowna, BC (Canada)',\n",
        "                     'Birmingham, AL (US)',\n",
        "                     'Alcobendas (Spain)',\n",
        "                     'Winslow, ME (US)',\n",
        "                     'Columbus, OH (US)',\n",
        "                     'Didcot, England',\n",
        "                     'South Plainfield, NJ (US)',\n",
        "                     'Manchester, England',\n",
        "                     'Huntsville, AL (US)',\n",
        "                     'Dartmouth, NS (Canada)',\n",
        "                     'Anchorage, AK (US)',\n",
        "                     'Bracknell, England',\n",
        "                     'Palo Alto, CA (US)',\n",
        "                     'Milpitas, CA (US)',\n",
        "                     'Raleigh, NC (US)',\n",
        "                     'Montevideo (Uruguay)',\n",
        "                     'Nizhniy Novgorod (Russia)',\n",
        "                     'Ta’if (Saudi Arabia)',\n",
        "                     'Kitchener, ON (Canada)',\n",
        "                     'Parkville (Australia)',\n",
        "                     'Täby, Stockholm (Sweden)',\n",
        "                     'Burnaby, BC (Canada)',\n",
        "                     'Turlock, CA (US)',\n",
        "                     'Senneville, QC (Canada)',\n",
        "                     'Maadi Cornish (Egypt)',\n",
        "                     'Dorchester, MA (US)',\n",
        "                     'Andros Town (Bahamas)',\n",
        "                     'Liverpool, England',\n",
        "                     'Exeter, England',\n",
        "                     'Plzeň (Czech Republic)',\n",
        "                     'Saskatoon, SK (Canada)',\n",
        "                     'Roxby Downs (Australia)',\n",
        "                     'Niles, IL (US)',\n",
        "                     'Warner Robins, GA (US)',\n",
        "                     'Leixlip (Ireland)',\n",
        "                     'Liberty Lake, WA (US)',\n",
        "                     'Champaign, IL (US)',\n",
        "                     'Guilford, CT (US)',\n",
        "                     'Lenexa, KS (US)',\n",
        "                     'San Diego, CA (US)',\n",
        "                     'Edendale (New Zealand)',\n",
        "                     'Danbury, CT (US)',\n",
        "                     'Dokki (Egypt)',\n",
        "                     'Victoria, BC (Canada)',\n",
        "                     'Bloomington, IL (US)',\n",
        "                     'Foster City, CA (US)',\n",
        "                     'Paramaribo (Suriname)',\n",
        "                     'Richmond, BC (Canada)',\n",
        "                     'Colorado Springs, CO (US)',\n",
        "                     'Oxford, England',\n",
        "                     'Baltimore, MD (US)',\n",
        "                     'Minneapolis, MN (US)',\n",
        "                     'Carlisle, England',\n",
        "                     'Altrincham, England',\n",
        "                     'Gainesville, FL (US)',\n",
        "                     'Fredericton, NB (Canada)',\n",
        "                     'Redwood City, CA (US)',\n",
        "                     'Dayton, OH (US)',\n",
        "                     'Lanett, AL (US)',\n",
        "                     'Cheltenham, England',\n",
        "                     'Bloemfontein (South Africa)',\n",
        "                     'Naas (Ireland)',\n",
        "                     'Watertown, MA (US)',\n",
        "                     'Duluth, GA (US)',\n",
        "                     'Burlington, ON (Canada)',\n",
        "                     'Kilkenny (Ireland)',\n",
        "                     'Columbia, MD (US)',\n",
        "                     'Palm Springs, FL (US)',\n",
        "                     'Halifax, NS (Canada)',\n",
        "                     'Hamilton, Waikato (New Zealand)',\n",
        "                     'Phoenix, AZ (US)',\n",
        "                     'Hatfield, East of England, England',\n",
        "                     'Anjou, QC (Canada)',\n",
        "                     'Lacey, WA (US)',\n",
        "                     'Kirkland, QC (Canada)',\n",
        "                     'Indianapolis, IN (US)',\n",
        "                     'Temple Terrace, FL (US)',\n",
        "                     'Denver, CO (US)',\n",
        "                     'Annapolis Junction, MD (US)',\n",
        "                     'Ebeye (Marshall Islands)',\n",
        "                     'Wilmington, DE (US)',\n",
        "                     'Horsham, England',\n",
        "                     'Pine Grove, PA (US)',\n",
        "                     'Kingston (Jamaica)',\n",
        "                     'McLean, VA (US)',\n",
        "                     'Whangarei, Northland (New Zealand)',\n",
        "                     'Geelong (Australia)',\n",
        "                     'Coventry, England',\n",
        "                     'Townsville (Australia)',\n",
        "                     'Bathgate, Scotland',\n",
        "                     'Houston, TX (US)',\n",
        "                     'Durban (South Africa)',\n",
        "                     'Chorley, England',\n",
        "                     'Reading, England',\n",
        "                     'Okemos, MI (US)',\n",
        "                     'Round Rock, TX (US)',\n",
        "                     'Stoughton, MA (US)',\n",
        "                     'Deerfield, IL (US)',\n",
        "                     'London, ON (Canada)',\n",
        "                     'Springfield, VA (US)',\n",
        "                     'Swindon, Wiltshire, South West England, England',\n",
        "                     'Croydon, Victoria (Australia)',\n",
        "                     'Whippany, NJ (US)',\n",
        "                     'Rosemead, CA (US)',\n",
        "                     'Las Vegas, NV (US)',\n",
        "                     'Saint Paul, MN (US)',\n",
        "                     'Capitol Heights, MD (US)',\n",
        "                     'Bellingham, WA (US)',\n",
        "                     'Roxburgh (New Zealand)',\n",
        "                     'Gibraltar (Gibraltar)',\n",
        "                     'Conshohocken, PA (US)',\n",
        "                     'Codsall, England',\n",
        "                     'Oak Brook, IL (US)',\n",
        "                     'Northampton, England',\n",
        "                     'Johns Creek, GA (US)',\n",
        "                     'Boucherville, QC (Canada)',\n",
        "                     'Hillsboro, OR (US)',\n",
        "                     'Dundalk (Ireland)',\n",
        "                     'Al Aḩmadī (Kuwait)',\n",
        "                     'Al ‘Udaylīyah (Kuwait)',\n",
        "                     'New Orleans, LA (US)',\n",
        "                     'Lowell, AR (US)',\n",
        "                     'Warton, Lancashire, Lancashire, North West England, England',\n",
        "                     'Miami Lakes, FL (US)',\n",
        "                     'Sherwood, AR (US)',\n",
        "                     'Princeton, NJ (US)',\n",
        "                     'Newcastle (South Africa)',\n",
        "                     'Valencia, CA (US)',\n",
        "                     'San Luis Obispo, CA (US)',\n",
        "                     'Bedford, NS (Canada)',\n",
        "                     'Chatswood (Australia)',\n",
        "                     'Dún Laoghaire (Ireland)',\n",
        "                     'Cheadle, West Midlands, England',\n",
        "                     'Mountain View, CA (US)',\n",
        "                     'Saint Louis, MO (US)',\n",
        "                     'Kingston, ON (Canada)',\n",
        "                    ]"
      ],
      "id": "OBgKL2gNtgUs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rf22gGrTtgUz"
      },
      "source": [
        "#select friom index location titles to keep in scope\n",
        "glassdoor_reviews_df = glassdoor_reviews_df.loc[locations_to_keep]\n",
        "glassdoor_reviews_df.reset_index(inplace=True)"
      ],
      "id": "rf22gGrTtgUz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmFQO8HMtgUz"
      },
      "source": [
        "#Create a new column with a concatenated review (combo of all review text fields)\n",
        "full_review_columns = ['full_review', 'Rating_overall']\n",
        "full_df = pd.DataFrame(columns=full_review_columns)"
      ],
      "id": "SmFQO8HMtgUz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPlFbT4ftgU0"
      },
      "source": [
        "#Create new dataframe consisting of all review text and overall ratings\n",
        "full_review_column = glassdoor_reviews_df['reviews.val.title'].astype(str) + ' ' + glassdoor_reviews_df['reviews.val.pros'].astype(str) + ' ' + glassdoor_reviews_df['reviews.val.cons'].astype(str) + ' ' + glassdoor_reviews_df['reviews.val.adviceToManagement'].astype(str)\n",
        "full_df.full_review = full_review_column\n",
        "full_df.Rating_overall = glassdoor_reviews_df['reviews.val.reviewRatings.overall']\n",
        "full_df"
      ],
      "id": "lPlFbT4ftgU0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "FYw99uXTtgU0"
      },
      "source": [
        "# create labels\n",
        "# review_columns = ['OneStar', 'TwoStars', 'ThreeStars', 'FourStars', 'FiveStars']\n",
        "# review_labels = pd.DataFrame(columns=review_columns)\n",
        "# fill frame \n",
        "# review_labels.OneStar = pros_dummies['Ratings_overall_1.0']\n",
        "# review_labels.TwoStars = pros_dummies['Ratings_overall_2.0']\n",
        "# review_labels.ThreeStars = pros_dummies['Ratings_overall_3.0']\n",
        "# review_labels.FourStars = pros_dummies['Ratings_overall_4.0']\n",
        "# review_labels.FiveStars = pros_dummies['Ratings_overall_5.0']\n",
        "# review_labels"
      ],
      "id": "FYw99uXTtgU0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3P2tPBftgU0"
      },
      "source": [
        "\n",
        "#### Shape data\n",
        "starting with simple subset of data - to be expanded as time allows"
      ],
      "id": "g3P2tPBftgU0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3DIaND7tgU0"
      },
      "source": [
        "#define variables for different dataframes (isolating text fields)\n",
        "\n",
        "#create simple dataframes with pro review text and overall rating\n",
        "pro_df_columns = ['pros', 'Rating_overall']\n",
        "pro_df = pd.DataFrame(columns=pro_df_columns)\n",
        "#fill frame \n",
        "pro_df.pros = glassdoor_reviews_df['reviews.val.pros']\n",
        "pro_df.Rating_overall = glassdoor_reviews_df['reviews.val.reviewRatings.overall']\n",
        "pro_df.dropna(inplace=True)\n",
        "pro_df.shape"
      ],
      "id": "F3DIaND7tgU0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jq9mMqR2yNZ"
      },
      "source": [
        "pro_df.head()"
      ],
      "id": "7jq9mMqR2yNZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfSnIhDvtgU0"
      },
      "source": [
        "#create simple dataframes with con review text and overall rating\n",
        "con_df_columns = ['cons', 'Rating_overall']\n",
        "con_df = pd.DataFrame(columns=con_df_columns)\n",
        "#fill frame \n",
        "con_df.cons = glassdoor_reviews_df['reviews.val.cons']\n",
        "con_df.Rating_overall = glassdoor_reviews_df['reviews.val.reviewRatings.overall']\n",
        "con_df.dropna(inplace=True)\n",
        "con_df.shape"
      ],
      "id": "AfSnIhDvtgU0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiGABWCWtgU1"
      },
      "source": [
        "#create simple dataframes with advice to management text and overall rating\n",
        "advice_df_columns = ['advice', 'Rating_overall']\n",
        "advice_df = pd.DataFrame(columns=advice_df_columns)\n",
        "#fill frame \n",
        "advice_df.advice = glassdoor_reviews_df['reviews.val.adviceToManagement']\n",
        "advice_df.Rating_overall = glassdoor_reviews_df['reviews.val.reviewRatings.overall']\n",
        "advice_df.dropna(inplace=True)\n",
        "advice_df.shape"
      ],
      "id": "NiGABWCWtgU1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFBC2MUdtgU1"
      },
      "source": [
        "#create simple dataframes with reveiw title text and overall rating\n",
        "title_df_columns = ['title', 'Rating_overall']\n",
        "title_df = pd.DataFrame(columns=title_df_columns)\n",
        "#fill frame \n",
        "title_df.title = glassdoor_reviews_df['reviews.val.title']\n",
        "title_df.Rating_overall = glassdoor_reviews_df['reviews.val.reviewRatings.overall']\n",
        "title_df.dropna(inplace=True)\n",
        "title_df.shape"
      ],
      "id": "YFBC2MUdtgU1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_dgF50htgU1"
      },
      "source": [
        "full_df"
      ],
      "id": "C_dgF50htgU1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PyQTyv8tgU1"
      },
      "source": [
        "## EDA"
      ],
      "id": "-PyQTyv8tgU1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyaV_HjhtgU2"
      },
      "source": [
        "full_df.shape"
      ],
      "id": "fyaV_HjhtgU2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLsSKg3CtgU2"
      },
      "source": [
        "full_df.value_counts(full_df['Rating_overall'])"
      ],
      "id": "bLsSKg3CtgU2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_4DSEXGtgU2"
      },
      "source": [
        "full_df.Rating_overall.value_counts().plot(kind='bar')\n",
        "plt.xlabel('stars')"
      ],
      "id": "l_4DSEXGtgU2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeIB9LMdtgU2"
      },
      "source": [
        "full_df.Rating_overall.value_counts().plot(kind='pie')"
      ],
      "id": "TeIB9LMdtgU2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkho1dv6tgU2"
      },
      "source": [
        "## Experiment 1 and 2:\n",
        "TF-IDF with unscaled and scaled continuous features\n",
        "Models: \n",
        "SVM\n",
        "LSTM with scaled TF-IDF features\n"
      ],
      "id": "nkho1dv6tgU2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHN3wWJTtgU3"
      },
      "source": [
        "# Count the total number of reviews in the reviews.val.pros column\n",
        "print(f\"Total number of docs in the corpus: {len(full_df.full_review)}\")"
      ],
      "id": "YHN3wWJTtgU3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "HELW2i_6tgU3"
      },
      "source": [
        "#define text processing libraries\n",
        "review = full_df.full_review\n",
        "rating = pro_df.Rating_overall\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "sw = stopwords.words('english')"
      ],
      "id": "HELW2i_6tgU3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-j2rY427tgU3"
      },
      "source": [
        "full_df.head()"
      ],
      "id": "-j2rY427tgU3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHx0tfkDtgU3"
      },
      "source": [
        "#define text processor- remove punctuation, convert to lowercase and eliminate stopwords\n",
        "#def process_text(doc):\n",
        "#   sw = set(stopwords.words('english'))\n",
        "#    regex = re.compile(\"[^a-zA-Z ]\")\n",
        "#    re_clean = regex.sub('', doc)\n",
        "##    words = word_tokenize(doc)\n",
        " #   lem = [lemmatizer.lemmatize(word) for word in doc]\n",
        " #   output = [word.lower() for word in lem if word.lower() not in sw]\n",
        " #   return output"
      ],
      "id": "iHx0tfkDtgU3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gezxrx4e4UgB"
      },
      "source": [
        "def process_words(doc):\n",
        "    # remove stopwords\n",
        "    sw = stopwords.words('english')\n",
        "    words = [word for word in doc if word.lower() not in sw]\n",
        "    word = \" \".join(words)\n",
        "\n",
        "    # Remove punctuations and numbers\n",
        "    word = re.sub('[^a-zA-Z]', ' ', doc)\n",
        "\n",
        "    # lemmatize\n",
        "    lem = [lemmatizer.lemmatize(word) for word in doc]\n",
        "\n",
        "    # Single character removal\n",
        "    word = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', word)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    word = re.sub(r'\\s+', ' ', word)\n",
        "\n",
        "    # lowercase\n",
        "    word = word.lower()\n",
        "\n",
        "    return word"
      ],
      "id": "gezxrx4e4UgB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKhElw783y_x"
      },
      "source": [
        "pro_df.head()"
      ],
      "id": "gKhElw783y_x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9-wtYt0tgU4"
      },
      "source": [
        "processed = process_words(pro_df.pros[0])\n",
        "print(processed)"
      ],
      "id": "G9-wtYt0tgU4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpJn3k3ItgU4"
      },
      "source": [
        "#process text in full review column\n",
        "full_df.full_review = full_df.full_review.apply(lambda doc: process_words(doc))\n",
        "full_df.head()"
      ],
      "id": "OpJn3k3ItgU4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCFVI_NMtgU4"
      },
      "source": [
        "# Create the CountVectorizer instance defining the stopwords in English to be ignored\n",
        "vectorizer = CountVectorizer(stop_words=\"english\")"
      ],
      "id": "pCFVI_NMtgU4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fplPiQlnyFYD"
      },
      "source": [
        "# create tfidf vectorizer\n",
        "tfidf = TfidfVectorizer(max_features=12000, ngram_range=(1,5), analyzer='char')"
      ],
      "id": "fplPiQlnyFYD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FcZZ2YI14YE"
      },
      "source": [
        "X = tfidf.fit_transform(full_df['full_review'])"
      ],
      "id": "6FcZZ2YI14YE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEWIkF4cSUv1"
      },
      "source": [
        "y = full_df['Rating_overall']"
      ],
      "id": "jEWIkF4cSUv1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvzvXrNO28Zc"
      },
      "source": [
        "X.shape"
      ],
      "id": "EvzvXrNO28Zc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hTIo2T128i7"
      },
      "source": [
        "y.shape"
      ],
      "id": "_hTIo2T128i7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTVs68mK841M"
      },
      "source": [
        "#split data into test and training sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
      ],
      "id": "pTVs68mK841M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_yn9TFA9Q7R"
      },
      "source": [
        "#check shape of datasets\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "id": "D_yn9TFA9Q7R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWZ7LezC9wl1"
      },
      "source": [
        "#create linear svc model\n",
        "clf = LinearSVC(C=20, class_weight='balanced')\n",
        "clf.fit(X_train, y_train)"
      ],
      "id": "HWZ7LezC9wl1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_Pp-I3R-N-A"
      },
      "source": [
        "#create y prediction model\n",
        "y_pred = clf.predict(X_test)"
      ],
      "id": "F_Pp-I3R-N-A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_uxXNQ4-vpR"
      },
      "source": [
        "# check accuracy\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "id": "y_uxXNQ4-vpR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDPHwPmgUY23"
      },
      "source": [
        "#test with '1 star' example\n",
        "x = 'this job is really sucks bad i do not like it'\n",
        "vec = tfidf.transform([x])\n",
        "clf.predict(vec)"
      ],
      "id": "vDPHwPmgUY23",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_VN-40UWC5M"
      },
      "source": [
        "#test with '5 star' example\n",
        "x = 'this job is really great i love it'\n",
        "vec = tfidf.transform([x])\n",
        "clf.predict(vec)"
      ],
      "id": "g_VN-40UWC5M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tjzbn86Ak8Os"
      },
      "source": [
        "# Evaluate the model\n",
        "model.evaluate(X_test, y_test)"
      ],
      "id": "Tjzbn86Ak8Os",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPBiTOJcS0Zv"
      },
      "source": [
        "## EDA on terms frequency weights"
      ],
      "id": "YPBiTOJcS0Zv"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mCotXG6tgU4"
      },
      "source": [
        "# Get the tokenization and occurrence count\n",
        "X_corpus = vectorizer.fit_transform(full_df.full_review)"
      ],
      "id": "3mCotXG6tgU4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZVsTomMtgU4"
      },
      "source": [
        "\n",
        "# Getting matrix info\n",
        "print(f\"Matrix shape: {X_corpus.shape}\")\n",
        "print(f\"Total number of documents: {X_corpus.shape[0]}\")\n",
        "print(f\"Total number of unique words (tokens): {X_corpus.shape[1]}\")"
      ],
      "id": "kZVsTomMtgU4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzcUl1qVtgU5"
      },
      "source": [
        "# Retrieve unique words list\n",
        "words_corpus = vectorizer.get_feature_names()"
      ],
      "id": "vzcUl1qVtgU5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9bxLhCItgU5"
      },
      "source": [
        "# Getting the TF-IDF weight of each word in corpus as DataFrame\n",
        "words_corpus_df = pd.DataFrame(\n",
        "    list(zip(words_corpus, np.ravel(X_corpus.mean(axis=0)))), columns=[\"Word\", \"TF-IDF\"]\n",
        ")\n",
        "\n",
        "words_corpus_df = words_corpus_df.sort_values(by=[\"TF-IDF\"], ascending=False)"
      ],
      "id": "t9bxLhCItgU5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIDgAjDFNsLr"
      },
      "source": [
        "words_corpus_df = words_corpus_df.dropna()"
      ],
      "id": "eIDgAjDFNsLr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2S8KpmWttgU5"
      },
      "source": [
        "# Highest 50 TF-IDF scores\n",
        "words_corpus_df.head(50)"
      ],
      "id": "2S8KpmWttgU5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nu5De1EMtgU5"
      },
      "source": [
        "# Lowest 10 TF-IDF scores\n",
        "words_corpus_df.tail(50)"
      ],
      "id": "Nu5De1EMtgU5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "To0vnLFqDsa6"
      },
      "source": [
        "#wordcloud \n",
        "text = words_corpus_df.Word\n",
        "\n",
        "# Get TF-IDF weights\n",
        "\n",
        "print(words_corpus_df['TF-IDF'])\n",
        "s_word_freq = pd.Series(words_corpus_df['TF-IDF'])\n",
        "s_word_freq.index = words_corpus_df['Word']\n",
        "di_word_freq = s_word_freq.to_dict()\n",
        "\n",
        "#Create words and frequencies pairings\n",
        "print(\"---\")\n",
        "print(\"di_word_freq:\")\n",
        "for k,v in di_word_freq.items():\n",
        "  print(k,v)\n",
        "\n",
        "# Display the generated image:\n",
        "\n",
        "cloud = WordCloud(width=900, height=500).generate_from_frequencies(di_word_freq)\n",
        "plt.imshow(cloud)\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "id": "To0vnLFqDsa6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shOT4tI3XtSF"
      },
      "source": [
        "X_train.shape"
      ],
      "id": "shOT4tI3XtSF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzvOHHVPXtbN"
      },
      "source": [
        "X_test.shape"
      ],
      "id": "hzvOHHVPXtbN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYpqj9eAXtkP"
      },
      "source": [
        "y_train.shape"
      ],
      "id": "hYpqj9eAXtkP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju6J1YjFVSjJ"
      },
      "source": [
        "y_test.shape"
      ],
      "id": "ju6J1YjFVSjJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzB3E6ZqtgU7"
      },
      "source": [
        "#use LSTM with scaled features\n",
        "# Use the MinMaxScaler to scale data between 0 and 1.\n",
        "x_train_scaler = StandardScaler()\n",
        "x_test_scaler = StandardScaler()\n",
        "y_train_scaler = StandardScaler()\n",
        "y_test_scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler for the training Data\n",
        "x_train_scaler.fit(X_train)\n",
        "y_train_scaler.fit(y_train)\n",
        "\n",
        "# Scale the training data\n",
        "X_train = x_train_scaler.transform(X_train)\n",
        "y_train = y_train_scaler.transform(y_train)\n",
        "\n",
        "# Fit the scaler for the testing Data\n",
        "x_test_scaler.fit(X_test)\n",
        "y_test_scaler.fit(y_test)\n",
        "\n",
        "# Scale the y_test data\n",
        "X_test = x_test_scaler.transform(X_test)\n",
        "y_test = y_test_scaler.transform(y_test)"
      ],
      "id": "mzB3E6ZqtgU7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGuMm8HotgU7"
      },
      "source": [
        "# Reshape the features for the model\n",
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))"
      ],
      "id": "JGuMm8HotgU7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPpxgoyTtgU7"
      },
      "source": [
        "# Build the LSTM model. \n",
        "# The return sequences need to be set to True if you are adding additional LSTM layers, but \n",
        "# You don't have to do this for the final layer. \n",
        "# Note: The dropouts help prevent overfitting\n",
        "# Note: The input shape is the number of time steps and the number of indicators\n",
        "# Note: Batching inputs has a different input shape of Samples/TimeSteps/Features\n",
        "model = Sequential()\n",
        "model.add(LSTM(\n",
        "    units=30, return_sequences=True,\n",
        "    input_shape=(X_train.shape[1], 1)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(units=30, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(units=30))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1))"
      ],
      "id": "mPpxgoyTtgU7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waCfNyVVtgU7"
      },
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')"
      ],
      "id": "waCfNyVVtgU7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeS_jGGetgU8"
      },
      "source": [
        " # Summarize the model\n",
        "model.summary()"
      ],
      "id": "SeS_jGGetgU8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQyjEphhtgU8"
      },
      "source": [
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, shuffle=False, batch_size=1, verbose=1)"
      ],
      "id": "xQyjEphhtgU8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFWBIIm7tgU8"
      },
      "source": [
        "# Evaluate the model\n",
        "model.evaluate(X_test, y_test)"
      ],
      "id": "oFWBIIm7tgU8",
      "execution_count": null,
      "outputs": []
    }
  ]
}